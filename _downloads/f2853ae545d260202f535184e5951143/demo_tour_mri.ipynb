{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Tour of MRI functionality in DeepInverse\n\nThis example presents the various datasets, forward physics and models\navailable in DeepInverse for Magnetic Resonance Imaging (MRI) problems:\n\n-  Physics: :class:`deepinv.physics.MRI`,\n   :class:`deepinv.physics.MultiCoilMRI`,\n   :class:`deepinv.physics.DynamicMRI`\n-  Datasets: the full [FastMRI](https://fastmri.med.nyu.edu)_ dataset\n   :class:`deepinv.datasets.FastMRISliceDataset` and a lightweight,\n   easy-to-use subset\n   :class:`deepinv.datasets.SimpleFastMRISliceDataset`\n-  Models: :class:`deepinv.models.VarNet`\n   ([VarNet](https://onlinelibrary.wiley.com/doi/full/10.1002/mrm.26977)_/[E2E-VarNet](https://arxiv.org/abs/2004.06688)_),\n   :class:`deepinv.models.MoDL` (a simple\n   [MoDL](https://ieeexplore.ieee.org/document/8434321)_ unrolled\n   model)\n\nContents:\n\n1. Get started with FastMRI (singlecoil + multicoil)\n2. Train an accelerated MRI with neural networks\n3. Load raw FastMRI data (singlecoil + multicoil)\n4. Train using raw data\n5. Explore 3D MRI\n6. Explore dynamic MRI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import deepinv as dinv\nimport torch, torchvision\nfrom torch.utils.data import DataLoader\n\ndevice = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\"\nrng = torch.Generator(device=device).manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Get started with FastMRI\n\nYou can get started with our simple\n[FastMRI](https://fastmri.med.nyu.edu)_ mini slice subsets which provide\nquick, easy-to-use, in-memory datasets which can be used for simulation\nexperiments.\n\n.. important::\n\n   By using this dataset, you confirm that you have agreed to and signed the [FastMRI data use agreement](https://fastmri.med.nyu.edu/).\n\n.. seealso::\n\n  Datasets :class:`deepinv.datasets.FastMRISliceDataset` :class:`deepinv.datasets.SimpleFastMRISliceDataset`\n      We provide convenient datasets to easily load both raw and reconstructed FastMRI images.\n      You can download more data on the [FastMRI site](https://fastmri.med.nyu.edu/).\n\nLoad mini demo knee and brain datasets (original data is 320x320 but we resize to\n128 for speed):\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "transform = torchvision.transforms.Resize(128)\nknee_dataset = dinv.datasets.SimpleFastMRISliceDataset(\n    dinv.utils.get_data_home(),\n    anatomy=\"knee\",\n    transform=transform,\n    train=True,\n    download=True,\n)\nbrain_dataset = dinv.datasets.SimpleFastMRISliceDataset(\n    dinv.utils.get_data_home(),\n    anatomy=\"brain\",\n    transform=transform,\n    train=True,\n    download=True,\n)\n\nimg_size = knee_dataset[0].shape[-2:]  # (128, 128)\ndinv.utils.plot({\"knee\": knee_dataset[0], \"brain\": brain_dataset[0]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's start with single-coil MRI. We can define a constant Cartesian 4x\nundersampling mask by sampling once from a physics generator. The mask,\ndata and measurements will all be of shape ``(B, C, H, W)`` where\n``C=2`` is the real and imaginary parts.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "physics_generator = dinv.physics.generator.GaussianMaskGenerator(\n    img_size=img_size, acceleration=4, rng=rng, device=device\n)\nmask = physics_generator.step()[\"mask\"]\n\nphysics = dinv.physics.MRI(mask=mask, img_size=img_size, device=device)\n\ndinv.utils.plot(\n    {\n        \"x\": (x := next(iter(DataLoader(knee_dataset)))),\n        \"mask\": mask,\n        \"y\": physics(x).clamp(-1, 1),\n    }\n)\nprint(\"Shapes:\", x.shape, physics.mask.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can next generate an accelerated single-coil MRI measurement dataset. Let's use knees\nfor training and brains for testing.\n\nWe can also use the physics generator to randomly sample a new mask per\nsample, and save the masks alongside the measurements:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset_path = dinv.datasets.generate_dataset(\n    train_dataset=knee_dataset,\n    test_dataset=brain_dataset,\n    val_dataset=None,\n    physics=physics,\n    physics_generator=physics_generator,\n    save_physics_generator_params=True,\n    overwrite_existing=False,\n    device=device,\n    save_dir=dinv.utils.get_data_home(),\n    batch_size=1,\n)\n\ntrain_dataset = dinv.datasets.HDF5Dataset(\n    dataset_path, split=\"train\", load_physics_generator_params=True\n)\ntest_dataset = dinv.datasets.HDF5Dataset(\n    dataset_path, split=\"test\", load_physics_generator_params=True\n)\n\ndinv.utils.plot(\n    {\n        \"x0\": train_dataset[0][0],\n        \"mask0\": train_dataset[0][2][\"mask\"],\n        \"x1\": train_dataset[1][0],\n        \"mask1\": train_dataset[1][2][\"mask\"],\n    }\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also simulate multicoil MRI data. Either pass in ground-truth\ncoil maps, or pass an integer to simulate simple birdcage coil maps. The\nmeasurements ``y`` are now of shape ``(B, C, N, H, W)``, where ``N`` is\nthe coil-dimension.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mc_physics = dinv.physics.MultiCoilMRI(img_size=img_size, coil_maps=3, device=device)\n\ndinv.utils.plot(\n    {\n        \"x\": x,\n        \"mask\": mask,\n        \"coil_map_0\": mc_physics.coil_maps.abs()[:, 0, ...],\n        \"coil_map_1\": mc_physics.coil_maps.abs()[:, 1, ...],\n        \"coil_map_2\": mc_physics.coil_maps.abs()[:, 2, ...],\n        \"RSS\": mc_physics.A_adjoint_A(x, mask=mask, rss=True),\n    }\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Train an accelerated MRI problem with neural networks\n\nNext, we train a neural network to solve the MRI inverse problem. We provide various\nmodels specifically used for MRI reconstruction. These are unrolled\nnetworks which require a backbone denoiser, such as UNet or DnCNN:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "denoiser = dinv.models.UNet(\n    in_channels=2,\n    out_channels=2,\n    scales=2,\n)\n\ndenoiser = dinv.models.DnCNN(\n    in_channels=2,\n    out_channels=2,\n    pretrained=None,\n    depth=2,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These backbones can be used within specific MRI models, such as\n[VarNet](https://onlinelibrary.wiley.com/doi/full/10.1002/mrm.26977)_/[E2E-VarNet](https://arxiv.org/abs/2004.06688)_\nand [MoDL](https://ieeexplore.ieee.org/document/8434321)_, for which we provide implementations:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = dinv.models.VarNet(denoiser, num_cascades=2, mode=\"varnet\").to(device)\n\nmodel = dinv.models.MoDL(denoiser, num_iter=2).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have our architecture defined, we can train it with supervised or self-supervised (using Equivariant\nImaging) loss. We use the PSNR metric on the complex magnitude.\n\nFor the sake of speed in this example, we only use a very small 2-layer DnCNN inside an unrolled\nnetwork with 2 cascades, and train with 2 images for 1 epoch.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss = dinv.loss.SupLoss()\nloss = dinv.loss.EILoss(transform=dinv.transform.CPABDiffeomorphism())\n\ntrainer = dinv.Trainer(\n    model=model,\n    physics=physics,\n    optimizer=torch.optim.Adam(model.parameters()),\n    train_dataloader=(train_dataloader := DataLoader(train_dataset)),\n    metrics=dinv.metric.PSNR(complex_abs=True),\n    epochs=1,\n    show_progress_bar=False,\n    save_path=None,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To improve results in the case of this very short training, we start training from a pretrained model state (trained on 900 images):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "url = dinv.models.utils.get_weights_url(\n    model_name=\"demo\", file_name=\"demo_tour_mri.pth\"\n)\nckpt = torch.hub.load_state_dict_from_url(\n    url, map_location=lambda storage, loc: storage, file_name=\"demo_tour_mri.pth\"\n)\ntrainer.model.load_state_dict(ckpt[\"state_dict\"])  # load the state dict\ntrainer.optimizer.load_state_dict(ckpt[\"optimizer\"])  # load the optimizer state dict\n\nmodel = trainer.train()  # train the model\ntrainer.plot_images = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that our model is trained, we can test it. Notice that we improve the PSNR compared to the zero-filled\nreconstruction, both on the train (knee) set and the test (brain) set:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "_ = trainer.test(train_dataloader)\n\n_ = trainer.test(DataLoader(test_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load raw FastMRI data\n\nIt is also possible to use the raw data directly.\nThe raw multi-coil FastMRI data is provided as pairs of ``(x, y)`` where\n``y`` are the fully-sampled k-space measurements of arbitrary size, and\n``x`` are the cropped root-sum-square (RSS) magnitude reconstructions.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dinv.datasets.download_archive(\n    dinv.utils.get_image_url(\"demo_fastmri_brain_multicoil.h5\"),\n    dinv.utils.get_data_home() / \"brain\" / \"fastmri.h5\",\n)\n\ndataset = dinv.datasets.FastMRISliceDataset(\n    dinv.utils.get_data_home() / \"brain\", slice_index=\"middle\"\n)\n\nx, y = next(iter(DataLoader(dataset)))\n\nprint(\"Shapes:\", x.shape, y.shape)  # x (B, 1, W, W); y (B, C, N, H, W)\n\nimg_size, kspace_shape = x.shape[-2:], y.shape[-2:]\nn_coils = y.shape[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can relate ``x`` and ``y`` using our\n:class:`deepinv.physics.MultiCoilMRI` (note that since we are not\nprovided with the ground-truth coil-maps, we can only perform the\nadjoint operator).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "physics = dinv.physics.MultiCoilMRI(\n    img_size=img_size,\n    mask=torch.ones(kspace_shape),\n    coil_maps=torch.ones((n_coils,) + kspace_shape, dtype=torch.complex64),\n    device=device,\n)\n\nx_rss = physics.A_adjoint(y, rss=True, crop=True)\n\nassert torch.allclose(x, x_rss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train using raw data\n\nWe now use a mask generator to generate acceleration masks **on-the-fly**\n(online) during training. We use the E2E-VarNet model designed for\nmulticoil MRI. We do not perform coil sensitivity map estimation and\nsimply assume they are flat as above. To do this yourself, pass a model\nas the ``sensitivity_model`` parameter.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "physics_generator = dinv.physics.generator.GaussianMaskGenerator(\n    img_size=kspace_shape, acceleration=4, rng=rng, device=device\n)\n\nmodel = dinv.models.VarNet(denoiser, num_cascades=2, mode=\"e2e-varnet\").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that we require overriding the base\n:class:`deepinv.Trainer` to deal with raw measurements, as we\ndo not want to generate k-space measurements, only mask it.\n\n.. note ::\n\n   We require `loop_random_online_physics=True` and `shuffle=False` in the dataloader to ensure that each image is always matched with the same random mask at each iteration.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class RawFastMRITrainer(dinv.Trainer):\n    def get_samples_online(self, iterators, g):\n        # Get data\n        x, y = next(iterators[g])\n        x, y = x.to(self.device), y.to(self.device)\n\n        # Get physics\n        physics = self.physics[g]\n\n        # Generate random mask\n        params = self.physics_generator[g].step(\n            batch_size=y.size(0), img_size=y.shape[-2:]\n        )\n\n        # Generate measurements directly from raw measurements\n        y *= params[\"mask\"]\n\n        physics.update(**params)\n\n        return x, y, physics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also need to modify the metrics used to crop the model output when\ncomparing to the cropped magnitude RSS targets:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "transform = torchvision.transforms.Compose(\n    [\n        torchvision.transforms.CenterCrop(x.shape[-2:]),\n        dinv.metric.functional.complex_abs,\n    ]\n)\n\n\nclass CropMSE(dinv.metric.MSE):\n    def forward(self, x_net=None, x=None, *args, **kwargs):\n        return super().forward(transform(x_net), x, *args, **kwargs)\n\n\nclass CropPSNR(dinv.metric.PSNR):\n    def forward(self, x_net=None, x=None, *args, **kwargs):\n        return super().forward(transform(x_net), x, *args, **kwargs)\n\n\ntrainer = RawFastMRITrainer(\n    model=model,\n    physics=physics,\n    physics_generator=physics_generator,\n    online_measurements=True,\n    loop_random_online_physics=True,\n    losses=dinv.loss.SupLoss(metric=CropMSE()),\n    metrics=CropPSNR(),\n    optimizer=torch.optim.Adam(model.parameters()),\n    train_dataloader=DataLoader(dataset, shuffle=False),\n    epochs=1,\n    save_path=None,\n    show_progress_bar=False,\n)\n_ = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Explore 3D MRI\n\nWe can also simulate 3D MRI data.\nHere, we use a demo 3D brain volume of shape ``(181, 217, 181)`` from the\n[BrainWeb](https://brainweb.bic.mni.mcgill.ca/brainweb/) dataset\nand simulate 3D single-coil or multi-coil Fourier measurements using\n:class:`deepinv.physics.MRI` or\n:class:`deepinv.physics.MultiCoilMRI`.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = (\n    torch.from_numpy(\n        dinv.utils.demo.load_np_url(\n            \"https://huggingface.co/datasets/deepinv/images/resolve/main/brainweb_t1_ICBM_1mm_subject_0.npy?download=true\"\n        )\n    )\n    .unsqueeze(0)\n    .unsqueeze(0)\n    .to(device)\n)\nx = torch.cat([x, torch.zeros_like(x)], dim=1)  # add imaginary dimension\n\nprint(x.shape)  # (B, C, D, H, W) where D is depth\n\nphysics = dinv.physics.MultiCoilMRI(img_size=x.shape[1:], three_d=True, device=device)\nphysics = dinv.physics.MRI(img_size=x.shape[1:], three_d=True, device=device)\n\ndinv.utils.plot_ortho3D([x, physics(x)], titles=[\"x\", \"y\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Explore dynamic MRI\n\nFinally, we show how to use the dynamic MRI for image sequence data of\nshape ``(B, C, T, H, W)`` where ``T`` is the time dimension. Note that\nthis is also compatible with 3D MRI. We use dynamic MRI data from the\n[CMRxRecon](https://cmrxrecon.github.io/) challenge of cardiac cine\nsequences and load them using :class:`deepinv.datasets.CMRxReconSliceDataset`\nprovided in deepinv. We download demo data from the first patient\nincluding ground truth images, undersampled kspace, and associated masks:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dinv.datasets.download_archive(\n    dinv.utils.get_image_url(\"CMRxRecon.zip\"),\n    dinv.utils.get_data_home() / \"CMRxRecon.zip\",\n    extract=True,\n)\n\ndataset = dinv.datasets.CMRxReconSliceDataset(\n    dinv.utils.get_data_home() / \"CMRxRecon\",\n)\n\nx, y, params = next(iter(DataLoader(dataset)))\n\nprint(\n    f\"\"\"\n    Ground truth: {x.shape} (B, C, T, H, W)\n    Measurements: {y.shape}\n    Acc. mask: {params[\"mask\"].shape}\n\"\"\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dynamic MRI data is directly compatible with existing functionality.\nFor example, you can train with this data by passing the dataset to\n:class:`deepinv.Trainer`, which will automatically load in the data\n``x, y, params``. Or, you can use the data directly with the physics\n:class:`deepinv.physics.DynamicMRI`.\n\nYou can also pass in a custom k-t acceleration mask generator to\ngenerate random time-varying masks:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "physics_generator = dinv.physics.generator.EquispacedMaskGenerator(\n    img_size=x.shape[1:], acceleration=16, rng=rng, device=device\n)\nphysics = dinv.physics.DynamicMRI(img_size=(512, 256), device=device)\n\ndataset = dinv.datasets.CMRxReconSliceDataset(\n    dinv.utils.get_data_home() / \"CMRxRecon\",\n    mask_generator=physics_generator,\n    mask_dir=None,\n)\n\nx, y, params = next(iter(DataLoader(dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We provide a video plotting function, :class:`deepinv.utils.plot_videos`. Here, we\nvisualise t=5 frames of the ground truth ``x``, the mask, and the zero-filled\nreconstruction ``x_zf`` (and crop to square for better visibility):\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x_zf = physics.A_adjoint(y, **params)\n\ndinv.utils.plot(\n    {\n        f\"t={i}\": torch.cat([x[:, :, i], params[\"mask\"][:, :, i], x_zf[:, :, i]])[\n            ..., 128:384, :\n        ]\n        for i in range(5)\n    }\n)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}